"use strict";(self.webpackChunk=self.webpackChunk||[]).push([[7130],{3905:(e,t,a)=>{a.d(t,{Zo:()=>u,kt:()=>m});var n=a(7294);function o(e,t,a){return t in e?Object.defineProperty(e,t,{value:a,enumerable:!0,configurable:!0,writable:!0}):e[t]=a,e}function r(e,t){var a=Object.keys(e);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);t&&(n=n.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),a.push.apply(a,n)}return a}function i(e){for(var t=1;t<arguments.length;t++){var a=null!=arguments[t]?arguments[t]:{};t%2?r(Object(a),!0).forEach((function(t){o(e,t,a[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(a)):r(Object(a)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(a,t))}))}return e}function l(e,t){if(null==e)return{};var a,n,o=function(e,t){if(null==e)return{};var a,n,o={},r=Object.keys(e);for(n=0;n<r.length;n++)a=r[n],t.indexOf(a)>=0||(o[a]=e[a]);return o}(e,t);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);for(n=0;n<r.length;n++)a=r[n],t.indexOf(a)>=0||Object.prototype.propertyIsEnumerable.call(e,a)&&(o[a]=e[a])}return o}var p=n.createContext({}),s=function(e){var t=n.useContext(p),a=t;return e&&(a="function"==typeof e?e(t):i(i({},t),e)),a},u=function(e){var t=s(e.components);return n.createElement(p.Provider,{value:t},e.children)},d={inlineCode:"code",wrapper:function(e){var t=e.children;return n.createElement(n.Fragment,{},t)}},c=n.forwardRef((function(e,t){var a=e.components,o=e.mdxType,r=e.originalType,p=e.parentName,u=l(e,["components","mdxType","originalType","parentName"]),c=s(a),m=o,h=c["".concat(p,".").concat(m)]||c[m]||d[m]||r;return a?n.createElement(h,i(i({ref:t},u),{},{components:a})):n.createElement(h,i({ref:t},u))}));function m(e,t){var a=arguments,o=t&&t.mdxType;if("string"==typeof e||o){var r=a.length,i=new Array(r);i[0]=c;var l={};for(var p in t)hasOwnProperty.call(t,p)&&(l[p]=t[p]);l.originalType=e,l.mdxType="string"==typeof e?e:o,i[1]=l;for(var s=2;s<r;s++)i[s]=a[s];return n.createElement.apply(null,i)}return n.createElement.apply(null,a)}c.displayName="MDXCreateElement"},9715:(e,t,a)=>{a.r(t),a.d(t,{assets:()=>u,contentTitle:()=>p,default:()=>m,frontMatter:()=>l,metadata:()=>s,toc:()=>d});var n=a(7462),o=a(3366),r=(a(7294),a(3905)),i=["components"],l={id:"deploy-on-gpu",title:"GPU applications"},p=void 0,s={unversionedId:"deploy-on-gpu",id:"deploy-on-gpu",title:"GPU applications",description:"By default you do not have the permission to run applications on GPU, they need to be booked.",source:"@site/docs/deploy-on-gpu.md",sourceDirName:".",slug:"/deploy-on-gpu",permalink:"/docs/deploy-on-gpu",editUrl:"https://github.com/MaastrichtU-IDS/dsri-documentation/edit/master/website/docs/deploy-on-gpu.md",tags:[],version:"current",lastUpdatedBy:"Vincent Emonet",lastUpdatedAt:1653493642,formattedLastUpdatedAt:"5/25/2022",frontMatter:{id:"deploy-on-gpu",title:"GPU applications"},sidebar:"docs",previous:{title:"Monitor your applications",permalink:"/docs/guide-monitoring"},next:{title:"Deploy from a Dockerfile",permalink:"/docs/guide-dockerfile-to-openshift"}},u={},d=[{value:"Prepare your GPU workspace",id:"prepare-your-gpu-workspace",level:2},{value:"Enable GPU in your workspace",id:"enable-gpu-in-your-workspace",level:2},{value:"TensorBoard logs visualization",id:"tensorboard-logs-visualization",level:3},{value:"Increase the number of GPUs in your workspace",id:"increase-the-number-of-gpus-in-your-workspace",level:3},{value:"Install GPU drivers in any image",id:"install-gpu-drivers-in-any-image",level:2}],c={toc:d};function m(e){var t=e.components,a=(0,o.Z)(e,i);return(0,r.kt)("wrapper",(0,n.Z)({},c,a,{components:t,mdxType:"MDXLayout"}),(0,r.kt)("div",{className:"admonition admonition-warning alert alert--danger"},(0,r.kt)("div",{parentName:"div",className:"admonition-heading"},(0,r.kt)("h5",{parentName:"div"},(0,r.kt)("span",{parentName:"h5",className:"admonition-icon"},(0,r.kt)("svg",{parentName:"span",xmlns:"http://www.w3.org/2000/svg",width:"12",height:"16",viewBox:"0 0 12 16"},(0,r.kt)("path",{parentName:"svg",fillRule:"evenodd",d:"M5.05.31c.81 2.17.41 3.38-.52 4.31C3.55 5.67 1.98 6.45.9 7.98c-1.45 2.05-1.7 6.53 3.53 7.7-2.2-1.16-2.67-4.52-.3-6.61-.61 2.03.53 3.33 1.94 2.86 1.39-.47 2.3.53 2.27 1.67-.02.78-.31 1.44-1.13 1.81 3.42-.59 4.78-3.42 4.78-5.56 0-2.84-2.53-3.22-1.25-5.61-1.52.13-2.03 1.13-1.89 2.75.09 1.08-1.02 1.8-1.86 1.33-.67-.41-.66-1.19-.06-1.78C8.18 5.31 8.68 2.45 5.05.32L5.03.3l.02.01z"}))),"Book a GPU")),(0,r.kt)("div",{parentName:"div",className:"admonition-content"},(0,r.kt)("p",{parentName:"div"},(0,r.kt)("strong",{parentName:"p"},"By default you do not have the permission to run applications on GPU"),", they need to be booked."),(0,r.kt)("p",{parentName:"div"},"You can check the availability of our GPUs, and reserve GPU slots in our ",(0,r.kt)("a",{parentName:"p",href:"/gpu-booking"},"GPU booking calendar \ud83d\udcc5")))),(0,r.kt)("p",null,"We are using images provided by Nvidia, and optimized for GPU. We currently deployed Tensorflow and PyTorch with JupyterLab and VSCode, but any image available in the Nvidia catalog should be easy to deploy: ",(0,r.kt)("a",{parentName:"p",href:"https://ngc.nvidia.com/catalog/containers"},"https://ngc.nvidia.com/catalog/containers")),(0,r.kt)("p",null,"Checkout ",(0,r.kt)("a",{parentName:"p",href:"https://github.com/MaastrichtU-IDS/jupyterlab#jupyterlab-on-gpu"},"this documentation")," for more details on how we build the optimized docker images for the DSRI GPUs. Feel free to ",(0,r.kt)("a",{parentName:"p",href:"https://github.com/MaastrichtU-IDS/jupyterlab#extend-an-image"},"extend the images")," to your needs."),(0,r.kt)("h2",{id:"prepare-your-gpu-workspace"},"Prepare your GPU workspace"),(0,r.kt)("p",null,"Start a workspace in your DSRI project with all drivers and dependencies for accessing the GPUs already installed. The workspace is based on Ubuntu, and you will be able to access it using the JupyterLab web UI and VisualStudio Code in the browser."),(0,r.kt)("p",null,"You can deploy your GPU workspace from the catalog:"),(0,r.kt)("ol",null,(0,r.kt)("li",{parentName:"ol"},"Go to the ",(0,r.kt)("a",{parentName:"li",href:"https://console-openshift-console.apps.dsri2.unimaas.nl/catalog"},"Catalog web UI"),": ",(0,r.kt)("strong",{parentName:"li"},"Add to Project")," > ",(0,r.kt)("strong",{parentName:"li"},"Browse Catalog")),(0,r.kt)("li",{parentName:"ol"},'Filter the catalog for  "GPU"'),(0,r.kt)("li",{parentName:"ol"},"Choose one of the available templates: ",(0,r.kt)("strong",{parentName:"li"},"JupyterLab on GPU"),"."),(0,r.kt)("li",{parentName:"ol"},(0,r.kt)("strong",{parentName:"li"},"Follow the instructions")," to create the template in the DSRI web UI, all informations about the images you can use are provided there. The most notable is the base image you want to use for your workspace (",(0,r.kt)("inlineCode",{parentName:"li"},"cuda"),", ",(0,r.kt)("inlineCode",{parentName:"li"},"tensorflow")," or ",(0,r.kt)("inlineCode",{parentName:"li"},"pytorch"),")")),(0,r.kt)("p",null,"Access the workspace from the route created (the small arrow at the top right of your application bubble in the Topology page)."),(0,r.kt)("p",null,"You can now add your code and data in the persistent folder to be fully prepared when you will get access to the GPUs."),(0,r.kt)("p",null,"You can find more details on the images we use and how to extend them in this repository: ",(0,r.kt)("a",{parentName:"p",href:"https://github.com/MaastrichtU-IDS/jupyterlab#jupyterlab-on-gpu"},"https://github.com/MaastrichtU-IDS/jupyterlab#jupyterlab-on-gpu")),(0,r.kt)("div",{className:"admonition admonition-info alert alert--info"},(0,r.kt)("div",{parentName:"div",className:"admonition-heading"},(0,r.kt)("h5",{parentName:"div"},(0,r.kt)("span",{parentName:"h5",className:"admonition-icon"},(0,r.kt)("svg",{parentName:"span",xmlns:"http://www.w3.org/2000/svg",width:"14",height:"16",viewBox:"0 0 14 16"},(0,r.kt)("path",{parentName:"svg",fillRule:"evenodd",d:"M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z"}))),"Storage")),(0,r.kt)("div",{parentName:"div",className:"admonition-content"},(0,r.kt)("p",{parentName:"div"},"Use the ",(0,r.kt)("strong",{parentName:"p"},(0,r.kt)("inlineCode",{parentName:"strong"},"/workspace/persistent")," folder"),", which is the JupyterLab workspace, to store your code and data persistently. Note that loading data from the persistent storage will be slowly that what you might expected, this is due to the nature of the distributed storage. So try to optimize this part and avoid reloading multiple time your data, and let us know if it is too much of a problem, we have some solution to improve this"))),(0,r.kt)("h2",{id:"enable-gpu-in-your-workspace"},"Enable GPU in your workspace"),(0,r.kt)("p",null,"Once the GPU quotas has been granted to your project, you will receive a message on Slack, or email when it is done. You can then update your deployment to use the GPUs using this command (our deployment name is ",(0,r.kt)("inlineCode",{parentName:"p"},"jupyterlab-gpu")," in those 2 examples, change it to yours)"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},'oc patch dc/jupyterlab-gpu --type=json -p=\'[{"op": "replace", "path": "/spec/template/spec/containers/0/resources", "value": {"requests": {"nvidia.com/gpu": 1}, "limits": {"nvidia.com/gpu": 1}}}]\'\n')),(0,r.kt)("p",null,"You can use the following command in the terminal of your container on the DSRI to see your current GPU usage:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"nvidia-smi\n")),(0,r.kt)("p",null,"Later you can remove the GPU from your app (the pod will be restarted automatically):"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},'oc patch dc/jupyterlab-gpu --type=json -p=\'[{"op": "replace", "path": "/spec/template/spec/containers/0/resources", "value": {}]\'\n')),(0,r.kt)("h3",{id:"tensorboard-logs-visualization"},"TensorBoard logs visualization"),(0,r.kt)("p",null,"When using Tensorflow, you can use ",(0,r.kt)("a",{parentName:"p",href:"https://www.tensorflow.org/tensorboard"},(0,r.kt)("strong",{parentName:"a"},"TensorBoard \ud83d\udcc8"))," to explore your machine learning runs. It should be already pre-installed in our JupyterLab for GPU templates."),(0,r.kt)("p",null,"Follow the usual process to run tensorboard: ",(0,r.kt)("a",{parentName:"p",href:"https://www.tensorflow.org/tensorboard/get_started"},"https://www.tensorflow.org/tensorboard/get_started")),(0,r.kt)("ol",null,(0,r.kt)("li",{parentName:"ol"},(0,r.kt)("p",{parentName:"li"},"Add the tensorboard callback to your ",(0,r.kt)("inlineCode",{parentName:"p"},"model.fit()")," function")),(0,r.kt)("li",{parentName:"ol"},(0,r.kt)("p",{parentName:"li"},"Then start Tensorboard in the terminal with ",(0,r.kt)("inlineCode",{parentName:"p"},"tensorboard --logdir logs")," (change the directory depending on where the logs of your runs are stored), it should tell you that tensorboard as been started on port 6006")),(0,r.kt)("li",{parentName:"ol"},(0,r.kt)("p",{parentName:"li"},"Open the Tensorboard view from the JupyterLab welcome page"))),(0,r.kt)("h3",{id:"increase-the-number-of-gpus-in-your-workspace"},"Increase the number of GPUs in your workspace"),(0,r.kt)("p",null,"If you already have a application running using 1 GPU, and you have been granted a 2nd GPU to speed up your experiment you can easily upgrade the number of GPU used by your application:"),(0,r.kt)("p",null,"From the ",(0,r.kt)("strong",{parentName:"p"},"Topology")," view click on your application:"),(0,r.kt)("ol",null,(0,r.kt)("li",{parentName:"ol"},"Stop the application, by decreasing the number of pod to 0 (in the ",(0,r.kt)("strong",{parentName:"li"},"Details")," tab)"),(0,r.kt)("li",{parentName:"ol"},"Click on ",(0,r.kt)("strong",{parentName:"li"},"Options")," > ",(0,r.kt)("strong",{parentName:"li"},"Edit Deployment")," > in the YAML of the deployment search for ",(0,r.kt)("inlineCode",{parentName:"li"},"limits")," and change the number of GPU assigned to your deployment to 2:")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-yaml"},"          resources:\n            limits:\n              nvidia.com/gpu: '2'\n            requests:\n              nvidia.com/gpu: '2'\n")),(0,r.kt)("p",null,"You can also do it using the command line, make sure to stop the pod first, and replace ",(0,r.kt)("inlineCode",{parentName:"p"},"jupyterlab-gpu")," by your app name in this command:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},'oc patch dc/jupyterlab-gpu --type=json -p=\'[{"op": "replace", "path": "/spec/template/spec/containers/0/resources", "value": {"requests": {"nvidia.com/gpu": 2}, "limits": {"nvidia.com/gpu": 2}}}]\'\n')),(0,r.kt)("ol",{start:3},(0,r.kt)("li",{parentName:"ol"},"Restart the pod for your application (the same way you stopped it)")),(0,r.kt)("h2",{id:"install-gpu-drivers-in-any-image"},"Install GPU drivers in any image"),(0,r.kt)("p",null,"See the latest official ",(0,r.kt)("a",{parentName:"p",href:"https://nvidia.github.io/nvidia-container-runtime"},"Nvidia docs")," to install the ",(0,r.kt)("inlineCode",{parentName:"p"},"nvidia-container-runtime")," (all packages and drivers required to access the GPU from your application)"),(0,r.kt)("p",null,"Here is an example of commands to add to a debian based ",(0,r.kt)("inlineCode",{parentName:"p"},"Dockerfile")," to install the GPU drivers:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-dockerfile"},"RUN curl -s -L https://nvidia.github.io/nvidia-container-runtime/gpgkey | \\\n    apt-key add - \\ &&\n    distribution=$(. /etc/os-release;echo $ID$VERSION_ID) \\ &&\n    curl -s -L https://nvidia.github.io/nvidia-container-runtime/$distribution/nvidia-container-runtime.list | \nRUN apt-get update \\ &&\n    apt-get install -y nvidia-container-runtime\n")),(0,r.kt)("p",null,"Then, build your image in your DSRI project using ",(0,r.kt)("inlineCode",{parentName:"p"},"oc")," from the folder where your put the ",(0,r.kt)("inlineCode",{parentName:"p"},"Dockerfile")," (replace ",(0,r.kt)("inlineCode",{parentName:"p"},"custom-app-gpu")," by your app name):"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"oc new-build --name custom-app-gpu --binary\noc start-build custom-app-gpu --from-dir=. --follow --wait\noc new-app custom-app-gpu\n")),(0,r.kt)("p",null,"You will then need to edit the deployment to add the GPU NodeSelector, the ",(0,r.kt)("inlineCode",{parentName:"p"},"serviceAccountName: anyuid")," and add a persistent storage"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"oc edit custom-app-gpu\n")),(0,r.kt)("p",null,"See also: official ",(0,r.kt)("a",{parentName:"p",href:"https://docs.nvidia.com/cuda/cuda-installation-guide-linux/index.html#debian-installation"},"Nvidia docs for CUDA")))}m.isMDXComponent=!0}}]);