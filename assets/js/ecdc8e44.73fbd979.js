"use strict";(self.webpackChunk=self.webpackChunk||[]).push([[229],{3905:(e,t,a)=>{a.d(t,{Zo:()=>c,kt:()=>d});var r=a(7294);function n(e,t,a){return t in e?Object.defineProperty(e,t,{value:a,enumerable:!0,configurable:!0,writable:!0}):e[t]=a,e}function o(e,t){var a=Object.keys(e);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);t&&(r=r.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),a.push.apply(a,r)}return a}function p(e){for(var t=1;t<arguments.length;t++){var a=null!=arguments[t]?arguments[t]:{};t%2?o(Object(a),!0).forEach((function(t){n(e,t,a[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(a)):o(Object(a)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(a,t))}))}return e}function l(e,t){if(null==e)return{};var a,r,n=function(e,t){if(null==e)return{};var a,r,n={},o=Object.keys(e);for(r=0;r<o.length;r++)a=o[r],t.indexOf(a)>=0||(n[a]=e[a]);return n}(e,t);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);for(r=0;r<o.length;r++)a=o[r],t.indexOf(a)>=0||Object.prototype.propertyIsEnumerable.call(e,a)&&(n[a]=e[a])}return n}var s=r.createContext({}),i=function(e){var t=r.useContext(s),a=t;return e&&(a="function"==typeof e?e(t):p(p({},t),e)),a},c=function(e){var t=i(e.components);return r.createElement(s.Provider,{value:t},e.children)},u={inlineCode:"code",wrapper:function(e){var t=e.children;return r.createElement(r.Fragment,{},t)}},k=r.forwardRef((function(e,t){var a=e.components,n=e.mdxType,o=e.originalType,s=e.parentName,c=l(e,["components","mdxType","originalType","parentName"]),k=i(a),d=n,h=k["".concat(s,".").concat(d)]||k[d]||u[d]||o;return a?r.createElement(h,p(p({ref:t},c),{},{components:a})):r.createElement(h,p({ref:t},c))}));function d(e,t){var a=arguments,n=t&&t.mdxType;if("string"==typeof e||n){var o=a.length,p=new Array(o);p[0]=k;var l={};for(var s in t)hasOwnProperty.call(t,s)&&(l[s]=t[s]);l.originalType=e,l.mdxType="string"==typeof e?e:n,p[1]=l;for(var i=2;i<o;i++)p[i]=a[i];return r.createElement.apply(null,p)}return r.createElement.apply(null,a)}k.displayName="MDXCreateElement"},8717:(e,t,a)=>{a.r(t),a.d(t,{assets:()=>c,contentTitle:()=>s,default:()=>d,frontMatter:()=>l,metadata:()=>i,toc:()=>u});var r=a(3117),n=a(102),o=(a(7294),a(3905)),p=["components"],l={id:"deploy-spark",title:"Spark cluster"},s=void 0,i={unversionedId:"deploy-spark",id:"deploy-spark",title:"Spark cluster",description:"To be able to deploy Spark you will need to ask the DSRI admins to enable the Spark Operator in your project. It will be done quickly, once enabled you will be able to start a Spark cluster in a few clicks.",source:"@site/docs/deploy-spark.md",sourceDirName:".",slug:"/deploy-spark",permalink:"/docs/deploy-spark",draft:!1,editUrl:"https://github.com/MaastrichtU-IDS/dsri-documentation/edit/master/website/docs/deploy-spark.md",tags:[],version:"current",lastUpdatedBy:"Vincent Emonet",lastUpdatedAt:1676546669,formattedLastUpdatedAt:"Feb 16, 2023",frontMatter:{id:"deploy-spark",title:"Spark cluster"},sidebar:"docs",previous:{title:"JupyterHub",permalink:"/docs/deploy-jupyterhub"},next:{title:"Run MPI jobs",permalink:"/docs/mpi-jobs"}},c={},u=[{value:"Deploy a Spark cluster",id:"deploy-a-spark-cluster",level:2},{value:"Deploy the cluster from the catalog",id:"deploy-the-cluster-from-the-catalog",level:3},{value:"Create a route to the Spark dashboard",id:"create-a-route-to-the-spark-dashboard",level:3},{value:"Run on Spark",id:"run-on-spark",level:2},{value:"Using PySpark",id:"using-pyspark",level:3},{value:"RDF analytics with SANSA and Zeppelin notebooks",id:"rdf-analytics-with-sansa-and-zeppelin-notebooks",level:3},{value:"Connect Spark to the persistent storage",id:"connect-spark-to-the-persistent-storage",level:2},{value:"Delete a running Spark cluster",id:"delete-a-running-spark-cluster",level:2}],k={toc:u};function d(e){var t=e.components,a=(0,n.Z)(e,p);return(0,o.kt)("wrapper",(0,r.Z)({},k,a,{components:t,mdxType:"MDXLayout"}),(0,o.kt)("admonition",{title:"Request access to the Spark Operator",type:"warning"},(0,o.kt)("p",{parentName:"admonition"},"To be able to deploy Spark you will need to ",(0,o.kt)("a",{parentName:"p",href:"/help"},"ask the DSRI admins")," to enable the Spark Operator in your project. It will be done quickly, once enabled you will be able to start a Spark cluster in a few clicks.")),(0,o.kt)("h2",{id:"deploy-a-spark-cluster"},"Deploy a Spark cluster"),(0,o.kt)("p",null,"Once the DSRI admins have enabled the Spark Operator your project, you should found a ",(0,o.kt)("strong",{parentName:"p"},"Spark Cluster")," entry in the Catalog (in the ",(0,o.kt)("strong",{parentName:"p"},"Operator Backed")," category)"),(0,o.kt)("h3",{id:"deploy-the-cluster-from-the-catalog"},"Deploy the cluster from the catalog"),(0,o.kt)("img",{src:"/img/screenshot-spark-operator1.png",alt:"Apache Spark in the Catalog",style:{maxWidth:"100%",maxHeight:"100%"}}),(0,o.kt)("p",null,"Click on the ",(0,o.kt)("strong",{parentName:"p"},"Spark Cluster")," entry to deploy a Spark cluster."),(0,o.kt)("p",null,"You will be presented a form where you can provide the number of Spark workers in your cluster. "),(0,o.kt)("p",null,"Additionally you can provide a label which can be helpful later to manage or delete the cluster, use the name of your application and the label ",(0,o.kt)("inlineCode",{parentName:"p"},"app"),", e.g.: ",(0,o.kt)("inlineCode",{parentName:"p"},"app=my-spark-cluster")),(0,o.kt)("img",{src:"/img/screenshot-spark-operator2.png",alt:"Deploy a Apache Spark cluster",style:{maxWidth:"100%",maxHeight:"100%"}}),(0,o.kt)("admonition",{title:"Change ",type:"tip"},(0,o.kt)("p",{parentName:"admonition"},"The number of Spark workers can be easily updated later in the Spark deployment YAML file.")),(0,o.kt)("h3",{id:"create-a-route-to-the-spark-dashboard"},"Create a route to the Spark dashboard"),(0,o.kt)("p",null,"Once the cluster has been started you can create a route to access the Spark web UI:"),(0,o.kt)("p",null,"Go to ",(0,o.kt)("strong",{parentName:"p"},"Search")," > Click on ",(0,o.kt)("strong",{parentName:"p"},"Resources")," and search for ",(0,o.kt)("strong",{parentName:"p"},"Route")," > Click on ",(0,o.kt)("strong",{parentName:"p"},"Route")),(0,o.kt)("p",null,"You should now see the routes deployed in your project. Click on the button ",(0,o.kt)("strong",{parentName:"p"},"Create Route")),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},"Give a short meaningful name to your route, e.g. ",(0,o.kt)("inlineCode",{parentName:"li"},"my-spark-ui")),(0,o.kt)("li",{parentName:"ul"},"Keep Hostname and Path as it is"),(0,o.kt)("li",{parentName:"ul"},"Select the ",(0,o.kt)("strong",{parentName:"li"},"Service")," corresponding your Spark cluster suffixed with ",(0,o.kt)("inlineCode",{parentName:"li"},"-ui"),", e.g. ",(0,o.kt)("inlineCode",{parentName:"li"},"my-spark-cluster-ui")),(0,o.kt)("li",{parentName:"ul"},"Select the ",(0,o.kt)("strong",{parentName:"li"},"Target Port")," of the route, it should be 8080")),(0,o.kt)("p",null,"You can now access the Spark web UI at the generated URL to see which jobs are running and the nodes in your cluster."),(0,o.kt)("h2",{id:"run-on-spark"},"Run on Spark"),(0,o.kt)("p",null,"You can now start a spark-enabled JupyterLab, or any other spark-enabled applications, to use the Spark cluster deployed."),(0,o.kt)("h3",{id:"using-pyspark"},"Using PySpark"),(0,o.kt)("p",null,"The easiest is to use a Spark-enabled JupyterLab image, such as ",(0,o.kt)("a",{parentName:"p",href:"https://github.com/jupyter/docker-stacks/tree/master/pyspark-notebook"},"jupyter/pyspark-notebook")),(0,o.kt)("p",null,"But you can also use any image as long as you download the jar file, install all requirements, such as ",(0,o.kt)("inlineCode",{parentName:"p"},"pyspark"),", and set the right environment variable, such as ",(0,o.kt)("inlineCode",{parentName:"p"},"SPARK_HOME")),(0,o.kt)("p",null,"Connect to a Spark cluster deployed in the same project, replace ",(0,o.kt)("inlineCode",{parentName:"p"},"spark-cluster")," by your Spark cluster name:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},"from pyspark import SparkConf, SparkContext\nfrom pyspark.sql import SparkSession\n# Stop existing Spark Context\nspark = SparkSession.builder.master(\"spark://spark-cluster:7077\").getOrCreate()\nspark.sparkContext.stop()\n# Connect to the Spark cluster\nconf = SparkConf().setAppName('sansa').setMaster('spark://spark-cluster:7077') \nsc = SparkContext(conf=conf)\n\n# Run basic Spark test\nx = ['spark', 'rdd', 'example', 'sample', 'example'] \ny = sc.parallelize(x)\ny.collect()\n")),(0,o.kt)("h3",{id:"rdf-analytics-with-sansa-and-zeppelin-notebooks"},"RDF analytics with SANSA and Zeppelin notebooks"),(0,o.kt)("p",null,(0,o.kt)("a",{parentName:"p",href:"http://sansa-stack.net"},"SANSA")," is a big data engine for scalable processing of large-scale RDF  data. SANSA uses Spark, or Flink, which offer fault-tolerant, highly  available and scalable approaches to efficiently process massive sized  datasets. SANSA provides the facilities for Semantic data  representation, Querying, Inference, and Analytics."),(0,o.kt)("p",null,"Use the ",(0,o.kt)("strong",{parentName:"p"},"Zeppelin notebook for Spark")," template in the catalog to start a Spark-enabled Zeppelin notebook. You can find more information on the Zeppelin image at ",(0,o.kt)("a",{parentName:"p",href:"https://github.com/rimolive/zeppelin-openshift"},"https://github.com/rimolive/zeppelin-openshift")),(0,o.kt)("p",null,"Connect and test Spark in a Zeppelin notebook, replace ",(0,o.kt)("inlineCode",{parentName:"p"},"spark-cluster")," by your Spark cluster name:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},"%pyspark\nfrom pyspark import SparkConf, SparkContext\nfrom pyspark.sql import SparkSession\n# Stop existing Spark Context\nspark = SparkSession.builder.master(\"spark://spark-cluster:7077\").getOrCreate()\nspark.sparkContext.stop()\n# Connect to the Spark cluster\nconf = SparkConf().setAppName('sansa').setMaster('spark://spark-cluster:7077') \nsc = SparkContext(conf=conf)\n\n# Run basic Spark test\nx = [1, 2, 3, 4, 5] \ny = sc.parallelize(x)\ny.collect()\n")),(0,o.kt)("p",null,"You should see the job running in the Spark web UI, kill the job with the ",(0,o.kt)("strong",{parentName:"p"},"kill")," button in the Spark dashboard."),(0,o.kt)("p",null,"You can now start to run your workload on the Spark cluster"),(0,o.kt)("admonition",{title:"Reset a Zeppelin notebook",type:"info"},(0,o.kt)("p",{parentName:"admonition"},"Click on the cranked wheel in the top right of the note: ",(0,o.kt)("strong",{parentName:"p"},"Interpreter binding"),", and reset the interpreter")),(0,o.kt)("p",null,"Use the official ",(0,o.kt)("a",{parentName:"p",href:"https://github.com/SANSA-Stack/SANSA-Notebooks/tree/stack-merge/sansa-notebooks"},"SANSA notebooks examples")),(0,o.kt)("p",null,"See more examples:"),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("a",{parentName:"li",href:"https://github.com/rimolive/zeppelin-openshift"},"https://github.com/rimolive/zeppelin-openshift"))),(0,o.kt)("h2",{id:"connect-spark-to-the-persistent-storage"},"Connect Spark to the persistent storage"),(0,o.kt)("p",null,"Instructions available at ",(0,o.kt)("a",{parentName:"p",href:"https://github.com/rimolive/ceph-spark-integration"},"https://github.com/rimolive/ceph-spark-integration")),(0,o.kt)("p",null,"Requirements:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-bash"},"pip install boto\n")),(0,o.kt)("p",null,"Check the ",(0,o.kt)("a",{parentName:"p",href:"https://github.com/rimolive/ceph-spark-integration/blob/master/notebooks/ceph-example.ipynb"},"example notebook for Ceph storage")),(0,o.kt)("h2",{id:"delete-a-running-spark-cluster"},"Delete a running Spark cluster"),(0,o.kt)("p",null,"Get all objects part of the Spark cluster, change ",(0,o.kt)("inlineCode",{parentName:"p"},"app=spark-cluster")," to match your Spark cluster name:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-bash"},"oc get all,secret,configmaps --selector app=spark-cluster\n")),(0,o.kt)("p",null,"Then delete the Operator deployment from the OpenShift web UI overview."))}d.isMDXComponent=!0}}]);