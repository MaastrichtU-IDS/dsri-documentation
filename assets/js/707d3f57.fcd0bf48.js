"use strict";(self.webpackChunk=self.webpackChunk||[]).push([[248],{5680:(e,t,n)=>{n.d(t,{xA:()=>u,yg:()=>y});var a=n(6540);function o(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function r(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);t&&(a=a.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,a)}return n}function i(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?r(Object(n),!0).forEach((function(t){o(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):r(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function l(e,t){if(null==e)return{};var n,a,o=function(e,t){if(null==e)return{};var n,a,o={},r=Object.keys(e);for(a=0;a<r.length;a++)n=r[a],t.indexOf(n)>=0||(o[n]=e[n]);return o}(e,t);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);for(a=0;a<r.length;a++)n=r[a],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(o[n]=e[n])}return o}var p=a.createContext({}),s=function(e){var t=a.useContext(p),n=t;return e&&(n="function"==typeof e?e(t):i(i({},t),e)),n},u=function(e){var t=s(e.components);return a.createElement(p.Provider,{value:t},e.children)},d={inlineCode:"code",wrapper:function(e){var t=e.children;return a.createElement(a.Fragment,{},t)}},c=a.forwardRef((function(e,t){var n=e.components,o=e.mdxType,r=e.originalType,p=e.parentName,u=l(e,["components","mdxType","originalType","parentName"]),c=s(n),y=o,g=c["".concat(p,".").concat(y)]||c[y]||d[y]||r;return n?a.createElement(g,i(i({ref:t},u),{},{components:n})):a.createElement(g,i({ref:t},u))}));function y(e,t){var n=arguments,o=t&&t.mdxType;if("string"==typeof e||o){var r=n.length,i=new Array(r);i[0]=c;var l={};for(var p in t)hasOwnProperty.call(t,p)&&(l[p]=t[p]);l.originalType=e,l.mdxType="string"==typeof e?e:o,i[1]=l;for(var s=2;s<r;s++)i[s]=n[s];return a.createElement.apply(null,i)}return a.createElement.apply(null,n)}c.displayName="MDXCreateElement"},8954:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>u,contentTitle:()=>p,default:()=>y,frontMatter:()=>l,metadata:()=>s,toc:()=>d});var a=n(9668),o=n(1367),r=(n(6540),n(5680)),i=["components"],l={id:"deploy-on-gpu",title:"GPU applications"},p=void 0,s={unversionedId:"deploy-on-gpu",id:"deploy-on-gpu",title:"GPU applications",description:"GPUs on the DSRI can only be used by one workspace at a time, and there is a limited number of GPUs (8).",source:"@site/docs/deploy-on-gpu.md",sourceDirName:".",slug:"/deploy-on-gpu",permalink:"/docs/deploy-on-gpu",draft:!1,editUrl:"https://github.com/MaastrichtU-IDS/dsri-documentation/edit/master/website/docs/deploy-on-gpu.md",tags:[],version:"current",lastUpdatedBy:"Adekunle Onaopepo",lastUpdatedAt:1733702260,formattedLastUpdatedAt:"Dec 8, 2024",frontMatter:{id:"deploy-on-gpu",title:"GPU applications"},sidebar:"docs",previous:{title:"Monitor your applications",permalink:"/docs/guide-monitoring"},next:{title:"Deploy from a Dockerfile",permalink:"/docs/guide-dockerfile-to-openshift"}},u={},d=[{value:"Prepare your GPU workspace",id:"prepare-your-gpu-workspace",level:2},{value:"About the docker images",id:"about-the-docker-images",level:3},{value:"Deploy the workspace",id:"deploy-the-workspace",level:3},{value:"Prepare the workspace",id:"prepare-the-workspace",level:3},{value:"Enable the GPU",id:"enable-the-gpu",level:2},{value:"Disable the GPU",id:"disable-the-gpu",level:2},{value:"Increase the number of GPUs",id:"increase-the-number-of-gpus",level:2},{value:"Install GPU drivers in any image",id:"install-gpu-drivers-in-any-image",level:2}],c={toc:d};function y(e){var t=e.components,n=(0,o.A)(e,i);return(0,r.yg)("wrapper",(0,a.A)({},c,n,{components:t,mdxType:"MDXLayout"}),(0,r.yg)("p",null,"GPUs on the DSRI can only be used by one workspace at a time, and there is a limited number of GPUs (8)."),(0,r.yg)("p",null,"\u26a0\ufe0f We currently provide a free access to those GPUs, but with the growing demands for GPUs it might get more restricted. As consideration for others, and to help keep this system open, it is important to make a maximum use of your GPUs when you get access to them. "),(0,r.yg)("p",null,"Unfortunately job scheduling is currently not mature enough on Kubernetes, you can look into ",(0,r.yg)("a",{parentName:"p",href:"https://volcano.sh/en/"},"volcano.sh")," if you are interested, but it is still quite experimental."),(0,r.yg)("p",null,"To use the GPU on the DSRI you will go through this process:"),(0,r.yg)("ol",null,(0,r.yg)("li",{parentName:"ol"},"Deploy, prepare and debug your GPU workspace"),(0,r.yg)("li",{parentName:"ol"},(0,r.yg)("a",{parentName:"li",href:"/gpu-booking"},"Book a GPU")),(0,r.yg)("li",{parentName:"ol"},"Once the booking is done you will receive an email about your reservation, and more emails when it starts and before it ends"),(0,r.yg)("li",{parentName:"ol"},"Enable the GPU in workspace when your booking starts, and make the best use of it!")),(0,r.yg)("admonition",{title:"Book a GPU",type:"warning"},(0,r.yg)("p",{parentName:"admonition"},(0,r.yg)("strong",{parentName:"p"},"By default you do not have the permission to run applications on GPU"),", you need to make a reservation."),(0,r.yg)("p",{parentName:"admonition"},"You can check the availability of our GPUs, and reserve GPU slots in the ",(0,r.yg)("a",{parentName:"p",href:"/gpu-booking"},"GPU booking calendar \ud83d\udcc5"))),(0,r.yg)("h2",{id:"prepare-your-gpu-workspace"},"Prepare your GPU workspace"),(0,r.yg)("p",null,"You will first need to start your workspace without the GPU enabled, you can then prepare your experiments: clone the code, download the data, prepare scripts to install all requirements (the workspace will be restarted when you enable the GPU). "),(0,r.yg)("h3",{id:"about-the-docker-images"},"About the docker images"),(0,r.yg)("p",null,"We are mainly using images provided by Nvidia, with all required drivers and optimizations for GPU pre-installed. You can access the workspace with JupyterLab and VisualStudio Code in your browser, and install dependencies with ",(0,r.yg)("inlineCode",{parentName:"p"},"apt-get"),", ",(0,r.yg)("inlineCode",{parentName:"p"},"conda")," or ",(0,r.yg)("inlineCode",{parentName:"p"},"pip")," in the workspace."),(0,r.yg)("p",null,"We currently mainly use Tensorflow, PyTorch and CUDA, but any image available in the ",(0,r.yg)("a",{parentName:"p",href:"https://ngc.nvidia.com/catalog/containers"},"Nvidia catalog")," should be easy to deploy. Checkout ",(0,r.yg)("a",{parentName:"p",href:"https://github.com/MaastrichtU-IDS/jupyterlab#jupyterlab-on-gpu"},"this documentation")," for more details on how we build the optimized docker images for the DSRI GPUs. And feel free to ",(0,r.yg)("a",{parentName:"p",href:"https://github.com/MaastrichtU-IDS/jupyterlab#extend-an-image"},"extend the images")," to install any software you need."),(0,r.yg)("h3",{id:"deploy-the-workspace"},"Deploy the workspace"),(0,r.yg)("p",null,"You can easily deploy your GPU workspace from the DSRI catalog:"),(0,r.yg)("ol",null,(0,r.yg)("li",{parentName:"ol"},"Go to the ",(0,r.yg)("a",{parentName:"li",href:"https://console-openshift-console.apps.dsri2.unimaas.nl/catalog"},"DSRI Catalog web UI"),": Click on ",(0,r.yg)("strong",{parentName:"li"},"Add to Project"),", then ",(0,r.yg)("strong",{parentName:"li"},"Browse Catalog")),(0,r.yg)("li",{parentName:"ol"},'Search the catalog for  "GPU", and make sure the Template checkbox is enabled'),(0,r.yg)("li",{parentName:"ol"},"Choose the template: ",(0,r.yg)("strong",{parentName:"li"},"JupyterLab on GPU")),(0,r.yg)("li",{parentName:"ol"},"Follow the instructions to create the template in the DSRI web UI, all information about the images you can use are provided there. The most notable is the base image you want to use for your workspace (",(0,r.yg)("inlineCode",{parentName:"li"},"cuda"),", ",(0,r.yg)("inlineCode",{parentName:"li"},"tensorflow")," or ",(0,r.yg)("inlineCode",{parentName:"li"},"pytorch"),")")),(0,r.yg)("p",null,"Access the workspace from the route created (the small arrow at the top right of your application bubble in the Topology page)."),(0,r.yg)("h3",{id:"prepare-the-workspace"},"Prepare the workspace"),(0,r.yg)("p",null,"You can now add your code and data in the persistent folder to be fully prepared when you will get access to the GPUs."),(0,r.yg)("p",null,"You can install dependencies with ",(0,r.yg)("inlineCode",{parentName:"p"},"apt-get"),", ",(0,r.yg)("inlineCode",{parentName:"p"},"conda")," or ",(0,r.yg)("inlineCode",{parentName:"p"},"pip"),". We recommend your to use scripts stored in the persistent folder to easily install all your requirements, so you can reinstall them when we enable the GPU, as it restarts the workspace."),(0,r.yg)("p",null,"For more information on how to use ",(0,r.yg)("inlineCode",{parentName:"p"},"conda"),"/",(0,r.yg)("inlineCode",{parentName:"p"},"mamba")," to install new dependencies or complete environment (useful if you need to use a different version of python than the one installed by default) checkout ",(0,r.yg)("a",{parentName:"p",href:"/docs/deploy-jupyter#%EF%B8%8F-manage-dependencies-with-conda"},"this page"),". "),(0,r.yg)("p",null,"\u26a0\ufe0f We recommend you to also try and debug your code on small sample using the CPU before getting the GPU, this way you will be able to directly start long running task when you get the GPU, instead of losing time debugging your code (it's probably not going to work on the first try, you know it)."),(0,r.yg)("p",null,"You can find more details on the images we use and how to extend them ",(0,r.yg)("a",{parentName:"p",href:"https://github.com/MaastrichtU-IDS/jupyterlab#jupyterlab-on-gpu"},"in this repository"),"."),(0,r.yg)("admonition",{title:"Storage",type:"info"},(0,r.yg)("p",{parentName:"admonition"},"Use the ",(0,r.yg)("strong",{parentName:"p"},(0,r.yg)("inlineCode",{parentName:"strong"},"/workspace/persistent")," folder"),", which is the JupyterLab workspace, to store your code and data persistently. Note that loading data from the persistent storage will be slowly that what you might expected, this is due to the nature of the distributed storage. So try to optimize this part and avoid reloading multiple time your data, and let us know if it is too much of a problem, we have some solution to improve this")),(0,r.yg)("h2",{id:"enable-the-gpu"},"Enable the GPU"),(0,r.yg)("p",null,"You will receive an email when the GPU has been enabled in your project. You can then update your deployment to use the GPUs using either the ",(0,r.yg)("inlineCode",{parentName:"p"},"oc")," command-line tool, or by editing the deployment configuration from the web UI"),(0,r.yg)("ul",null,(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("strong",{parentName:"li"},"With the Command Line Interface"),", run the following command from the terminal of your laptop after having installed the ",(0,r.yg)("inlineCode",{parentName:"li"},"oc")," command-line tool.")),(0,r.yg)("p",null,"We use ",(0,r.yg)("inlineCode",{parentName:"p"},"jupyterlab-gpu")," as deployment name is  in the example, change it to yours if it is different."),(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre",className:"language-bash"},'oc patch dc/jupyterlab-gpu --type=json -p=\'[{"op": "replace", "path": "/spec/template/spec/containers/0/resources", "value": {"requests": {"nvidia.com/gpu": 1}, "limits": {"nvidia.com/gpu": 1}}}]\'\n')),(0,r.yg)("ul",null,(0,r.yg)("li",{parentName:"ul"},"Or ",(0,r.yg)("strong",{parentName:"li"},"through the web UI"))),(0,r.yg)("p",null,"In the ",(0,r.yg)("strong",{parentName:"p"},"Topology")," view click on the circle representing your GPU application, then click on the ",(0,r.yg)("strong",{parentName:"p"},"Actions")," button in the top right of the screen, and click on ",(0,r.yg)("strong",{parentName:"p"},"Edit Deployment Config")," at the bottom of the list"),(0,r.yg)("p",null,"In the Deployment Config text editor, hit ",(0,r.yg)("inlineCode",{parentName:"p"},"ctrl + f"),' to search for "',(0,r.yg)("strong",{parentName:"p"},"resources"),'". You should see a line ',(0,r.yg)("inlineCode",{parentName:"p"},"- resources: {}")," under ",(0,r.yg)("inlineCode",{parentName:"p"},"containers:"),". You need to change this line to the following to enable GPU in your application (and make sure the indentation match the rest of the file):"),(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre",className:"language-yaml"},"        - resources:\n            requests:\n              nvidia.com/gpu: 1\n            limits:\n              nvidia.com/gpu: 1\n")),(0,r.yg)("p",null,"Then wait for the pod to restart, or start it if it was stopped."),(0,r.yg)("p",null,"You can use the following command in the terminal of your container on the DSRI to see the current GPU usage:"),(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre",className:"language-bash"},"nvidia-smi\n")),(0,r.yg)("admonition",{title:"Windows",type:"info"},(0,r.yg)("p",{parentName:"admonition"},'When using above command with the oc client on windows you might receive an error like:\nerror: unable to parse "\'[{op:": yaml: found unexpected end of stream'),(0,r.yg)("p",{parentName:"admonition"},"This is because the single quotation mark on windows is handled differently. Try replacing the single quotation marks in the command with double quotation marks and the command should work.")),(0,r.yg)("h2",{id:"disable-the-gpu"},"Disable the GPU"),(0,r.yg)("p",null,"The GPU allocated to your workspace will be automatically disabled the after your booking ends at 9:00."),(0,r.yg)("p",null,"You can also manually disable the GPU from your app, the pod will be restarted automatically on a CPU node:"),(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre",className:"language-bash"},'oc patch dc/jupyterlab-gpu --type=json -p=\'[{"op": "replace", "path": "/spec/template/spec/containers/0/resources", "value": {}}]\'\n')),(0,r.yg)("h2",{id:"increase-the-number-of-gpus"},"Increase the number of GPUs"),(0,r.yg)("p",null,"If you have been granted a 2nd GPU to speed up your experiment you can easily upgrade the number of GPU used by your workspace:"),(0,r.yg)("p",null,"From the ",(0,r.yg)("strong",{parentName:"p"},"Topology")," view click on your application:"),(0,r.yg)("ol",null,(0,r.yg)("li",{parentName:"ol"},"Stop the application, by decreasing the number of pod to 0 (in the ",(0,r.yg)("strong",{parentName:"li"},"Details")," tab)"),(0,r.yg)("li",{parentName:"ol"},"Click on ",(0,r.yg)("strong",{parentName:"li"},"Options")," > ",(0,r.yg)("strong",{parentName:"li"},"Edit Deployment")," > in the YAML of the deployment search for ",(0,r.yg)("inlineCode",{parentName:"li"},"limits")," and change the number of GPU assigned to your deployment to 2:")),(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre",className:"language-yaml"},"          resources:\n            limits:\n              nvidia.com/gpu: '2'\n            requests:\n              nvidia.com/gpu: '2'\n")),(0,r.yg)("p",null,"You can also do it using the command line, make sure to stop the pod first, and replace ",(0,r.yg)("inlineCode",{parentName:"p"},"jupyterlab-gpu")," by your app name in this command:"),(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre",className:"language-bash"},'oc patch dc/jupyterlab-gpu --type=json -p=\'[{"op": "replace", "path": "/spec/template/spec/containers/0/resources", "value": {"requests": {"nvidia.com/gpu": 2}, "limits": {"nvidia.com/gpu": 2}}}]\'\n')),(0,r.yg)("ol",{start:3},(0,r.yg)("li",{parentName:"ol"},"Restart the pod for your application (the same way you stopped it)")),(0,r.yg)("h2",{id:"install-gpu-drivers-in-any-image"},"Install GPU drivers in any image"),(0,r.yg)("p",null,"You can also install the GPU drivers in any image and use this image directly."),(0,r.yg)("p",null,"See the latest official ",(0,r.yg)("a",{parentName:"p",href:"https://nvidia.github.io/nvidia-container-runtime"},"Nvidia docs")," to install the ",(0,r.yg)("inlineCode",{parentName:"p"},"nvidia-container-runtime"),", which should contain all packages and drivers required to access the GPU from your application."),(0,r.yg)("p",null,"Here is an example of commands to add to a debian based ",(0,r.yg)("inlineCode",{parentName:"p"},"Dockerfile")," to install the GPU drivers (note that this is not complete, you will need to check the latest instructions and do some research & development to get it to work):"),(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre",className:"language-dockerfile"},"RUN curl -s -L https://nvidia.github.io/nvidia-container-runtime/gpgkey | \\\n    apt-key add - \\ &&\n    distribution=$(. /etc/os-release;echo $ID$VERSION_ID) \\ &&\n    curl -s -L https://nvidia.github.io/nvidia-container-runtime/$distribution/nvidia-container-runtime.list | \nRUN apt-get update \\ &&\n    apt-get install -y nvidia-container-runtime\n")),(0,r.yg)("p",null,"Then, build your image in your DSRI project using ",(0,r.yg)("inlineCode",{parentName:"p"},"oc")," from the folder where your put the ",(0,r.yg)("inlineCode",{parentName:"p"},"Dockerfile")," (replace ",(0,r.yg)("inlineCode",{parentName:"p"},"custom-app-gpu")," by your app name):"),(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre",className:"language-bash"},"oc new-build --name custom-app-gpu --binary\noc start-build custom-app-gpu --from-dir=. --follow --wait\noc new-app custom-app-gpu\n")),(0,r.yg)("p",null,"You will then need to edit the deployment to the ",(0,r.yg)("inlineCode",{parentName:"p"},"serviceAccountName: anyuid")," and add a persistent storage"),(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre",className:"language-bash"},"oc edit custom-app-gpu\n")),(0,r.yg)("p",null,"Finally for when your reservation start, checkout the section above about how to enable the GPU in workspace "),(0,r.yg)("p",null,"See also: official ",(0,r.yg)("a",{parentName:"p",href:"https://docs.nvidia.com/cuda/cuda-installation-guide-linux/index.html#debian-installation"},"Nvidia docs for CUDA")))}y.isMDXComponent=!0}}]);