"use strict";(self.webpackChunk=self.webpackChunk||[]).push([[6108],{5680:(e,a,t)=>{t.d(a,{xA:()=>c,yg:()=>m});var n=t(6540);function r(e,a,t){return a in e?Object.defineProperty(e,a,{value:t,enumerable:!0,configurable:!0,writable:!0}):e[a]=t,e}function o(e,a){var t=Object.keys(e);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);a&&(n=n.filter((function(a){return Object.getOwnPropertyDescriptor(e,a).enumerable}))),t.push.apply(t,n)}return t}function s(e){for(var a=1;a<arguments.length;a++){var t=null!=arguments[a]?arguments[a]:{};a%2?o(Object(t),!0).forEach((function(a){r(e,a,t[a])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(t)):o(Object(t)).forEach((function(a){Object.defineProperty(e,a,Object.getOwnPropertyDescriptor(t,a))}))}return e}function i(e,a){if(null==e)return{};var t,n,r=function(e,a){if(null==e)return{};var t,n,r={},o=Object.keys(e);for(n=0;n<o.length;n++)t=o[n],a.indexOf(t)>=0||(r[t]=e[t]);return r}(e,a);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);for(n=0;n<o.length;n++)t=o[n],a.indexOf(t)>=0||Object.prototype.propertyIsEnumerable.call(e,t)&&(r[t]=e[t])}return r}var l=n.createContext({}),u=function(e){var a=n.useContext(l),t=a;return e&&(t="function"==typeof e?e(a):s(s({},a),e)),t},c=function(e){var a=u(e.components);return n.createElement(l.Provider,{value:a},e.children)},d={inlineCode:"code",wrapper:function(e){var a=e.children;return n.createElement(n.Fragment,{},a)}},p=n.forwardRef((function(e,a){var t=e.components,r=e.mdxType,o=e.originalType,l=e.parentName,c=i(e,["components","mdxType","originalType","parentName"]),p=u(t),m=r,g=p["".concat(l,".").concat(m)]||p[m]||d[m]||o;return t?n.createElement(g,s(s({ref:a},c),{},{components:t})):n.createElement(g,s({ref:a},c))}));function m(e,a){var t=arguments,r=a&&a.mdxType;if("string"==typeof e||r){var o=t.length,s=new Array(o);s[0]=p;var i={};for(var l in a)hasOwnProperty.call(a,l)&&(i[l]=a[l]);i.originalType=e,i.mdxType="string"==typeof e?e:r,s[1]=i;for(var u=2;u<o;u++)s[u]=t[u];return n.createElement.apply(null,s)}return n.createElement.apply(null,t)}p.displayName="MDXCreateElement"},984:(e,a,t)=>{t.r(a),t.d(a,{assets:()=>c,contentTitle:()=>l,default:()=>m,frontMatter:()=>i,metadata:()=>u,toc:()=>d});var n=t(9668),r=t(1367),o=(t(6540),t(5680)),s=["components"],i={id:"dask-tutorial",title:"Parallelization using Dask"},l=void 0,u={unversionedId:"dask-tutorial",id:"dask-tutorial",title:"Parallelization using Dask",description:"\ud83e\uddca Installation",source:"@site/docs/dask-tutorial.md",sourceDirName:".",slug:"/dask-tutorial",permalink:"/docs/dask-tutorial",draft:!1,editUrl:"https://github.com/MaastrichtU-IDS/dsri-documentation/edit/master/website/docs/dask-tutorial.md",tags:[],version:"current",lastUpdatedBy:"Laurent Winckers",lastUpdatedAt:1752832391,formattedLastUpdatedAt:"Jul 18, 2025",frontMatter:{id:"dask-tutorial",title:"Parallelization using Dask"},sidebar:"docs",previous:{title:"Checkpointing Machine Learning Training",permalink:"/docs/checkpointing-ml-training"},next:{title:"Introduction to workflows",permalink:"/docs/workflows-introduction"}},c={},d=[{value:"\ud83e\uddca Installation",id:"-installation",level:2},{value:"\ud83e\ude90 Basic Concepts of Dask",id:"-basic-concepts-of-dask",level:3},{value:"\u2728 Selecting columns and element-wise operations",id:"-selecting-columns-and-element-wise-operations",level:3},{value:"\u26a1\ufe0f Conditional filtering",id:"\ufe0f-conditional-filtering",level:3},{value:"\u2728 Common summary statistics",id:"-common-summary-statistics",level:3},{value:"\u2728 Groupby",id:"-groupby",level:3},{value:"\u26a1\ufe0f Lazy evaluation",id:"\ufe0f-lazy-evaluation",level:3},{value:"\ud83e\ude90 Dask Bags and Dask Delayed for Unstructured Data",id:"-dask-bags-and-dask-delayed-for-unstructured-data",level:4}],p={toc:d};function m(e){var a=e.components,t=(0,r.A)(e,s);return(0,o.yg)("wrapper",(0,n.A)({},p,t,{components:a,mdxType:"MDXLayout"}),(0,o.yg)("h2",{id:"-installation"},"\ud83e\uddca Installation"),(0,o.yg)("pre",null,(0,o.yg)("code",{parentName:"pre",className:"language-python"},'!pip install "dask[complete]"\n')),(0,o.yg)("pre",null,(0,o.yg)("code",{parentName:"pre",className:"language-python"},"import dask\n\ndask.__version__\n")),(0,o.yg)("pre",null,(0,o.yg)("code",{parentName:"pre"},"'2023.5.0'\n")),(0,o.yg)("pre",null,(0,o.yg)("code",{parentName:"pre",className:"language-python"},"import dask.array as da\nimport dask.bag as db\nimport dask.dataframe as dd\nimport numpy as np\nimport pandas as pd\n")),(0,o.yg)("h3",{id:"-basic-concepts-of-dask"},"\ud83e\ude90 Basic Concepts of Dask"),(0,o.yg)("p",null,"On a high-level, you can think of Dask as a wrapper that extends the capabilities of traditional tools like pandas, NumPy, and Spark to handle larger-than-memory datasets."),(0,o.yg)("p",null,"When faced with large objects like larger-than-memory arrays (vectors) or matrices (dataframes), Dask breaks them up into chunks, also called partitions."),(0,o.yg)("p",null,"For example, consider the array of 12 random numbers in both NumPy and Dask:"),(0,o.yg)("pre",null,(0,o.yg)("code",{parentName:"pre",className:"language-python"},"narr = np.random.rand(12)\n\nnarr\n")),(0,o.yg)("pre",null,(0,o.yg)("code",{parentName:"pre"},"array([0.44236558, 0.00504448, 0.87087911, 0.468925  , 0.37513511,\n       0.22607761, 0.83035297, 0.07772372, 0.61587933, 0.82861156,\n       0.66214299, 0.90979423])\n")),(0,o.yg)("pre",null,(0,o.yg)("code",{parentName:"pre",className:"language-python"},"darr = da.from_array(narr, chunks=3)\ndarr\n")),(0,o.yg)("img",{src:"/img/Screenshot-dask.png",alt:"dask table",style:{maxWidth:"100%",maxHeight:"100%"}}),(0,o.yg)("p",null,"The image above shows that the Dask array contains four chunks as we set chunks to 3. Under the hood, each chunk is a NumPy array in itself."),(0,o.yg)("p",null,"To fully appreciate the benefits of Dask, we need a large dataset, preferably over 1 GB in size. Consider the autogenerated data from the script below:"),(0,o.yg)("pre",null,(0,o.yg)("code",{parentName:"pre",className:"language-python"},"import string\n\n# Set the desired number of rows and columns\nnum_rows = 5_000_000\nnum_cols = 10\nchunk_size = 100_000\n\n# Define an empty DataFrame to store the chunks\ndf_chunks = pd.DataFrame()\n\n# Generate and write the dataset in chunks\nfor i in range(0, num_rows, chunk_size):\n    # Generate random numeric data\n    numeric_data = np.random.rand(chunk_size, num_cols)\n\n    # Generate random categorical data\n    letters = list(string.ascii_uppercase)\n    categorical_data = np.random.choice(letters, (chunk_size, num_cols))\n\n    # Combine numeric and categorical data into a Pandas DataFrame\n    df_chunk = pd.DataFrame(np.concatenate([numeric_data, categorical_data], axis=1))\n\n    # Set column names for better understanding\n    column_names = [f'Numeric_{i}' for i in range(num_cols)] + [f'Categorical_{i}' for i in range(num_cols)]\n    df_chunk.columns = column_names\n\n    # Append the current chunk to the DataFrame holding all chunks\n    df_chunks = pd.concat([df_chunks, df_chunk], ignore_index=True)\n\n    # Write the DataFrame chunk to a CSV file incrementally\n    if (i + chunk_size) >= num_rows or (i // chunk_size) % 10 == 0:\n        df_chunks.to_csv('large_dataset.csv', index=False, mode='a', header=(i == 0))\n        df_chunks = pd.DataFrame()\n")),(0,o.yg)("pre",null,(0,o.yg)("code",{parentName:"pre",className:"language-python"},'dask_df = dd.read_csv("large_dataset.csv")\n\ndask_df.head()\n')),(0,o.yg)("p",null,"Even though the file is large, you will notice that the result is fetched almost instantaneously. For even larger files, you can specify the ",(0,o.yg)("inlineCode",{parentName:"p"},"blocksize")," parameter, which determines the number of bytes to break up the file into."),(0,o.yg)("p",null,"Similar to how Dask Arrays contain chunks of small NumPy arrays, Dask is designed to handle multiple small Pandas DataFrames arranged along the row index."),(0,o.yg)("h3",{id:"-selecting-columns-and-element-wise-operations"},"\u2728 Selecting columns and element-wise operations"),(0,o.yg)("p",null,"In this example, we're doing some pretty straightforward column operations on our Dask DataFrame, called dask_df. We're adding the values from the column Numeric_0 to the result of multiplying the values from Numeric_9 and Numeric_3. We store the outcome in a variable named result."),(0,o.yg)("pre",null,(0,o.yg)("code",{parentName:"pre",className:"language-python"},'result = (\n   dask_df["Numeric_0"] + dask_df["Numeric_9"] * dask_df["Numeric_3"]\n)\n\nresult.compute().head()\n')),(0,o.yg)("p",null,"As we\u2019ve mentioned, Dask is a bit different from traditional computing tools in that it doesn't immediately execute these operations. Instead, it creates a kind of 'plan' called a task graph to carry out these operations later on. This approach allows Dask to optimize the computations and parallelize them when needed. The compute() function triggers Dask to finally perform these computations, and head() just shows us the first few rows of the result."),(0,o.yg)("h3",{id:"\ufe0f-conditional-filtering"},"\u26a1\ufe0f Conditional filtering"),(0,o.yg)("p",null,'Now, let\'s look at how Dask can filter data. We\'re selecting rows from our DataFrame where the value in the "Categorical_5" column is "A".'),(0,o.yg)("p",null,"This filtering process is similar to how you'd do it in pandas, but with a twist - Dask does this operation lazily. It prepares the task graph for this operation but waits to execute it until we call compute(). When we run head(), we get to see the first few rows of our filtered DataFrame."),(0,o.yg)("pre",null,(0,o.yg)("code",{parentName:"pre",className:"language-python"},'dask_df[dask_df["Categorical_5"] == "A"].compute().head()\n')),(0,o.yg)("h3",{id:"-common-summary-statistics"},"\u2728 Common summary statistics"),(0,o.yg)("p",null,"Next, we're going to generate some common summary statistics using Dask's describe() function."),(0,o.yg)("p",null,"It gives us a handful of descriptive statistics for our DataFrame, including the mean, standard deviation, minimum, maximum, and so on. As with our previous examples, Dask prepares the task graph for this operation when we call describe(), but it waits to execute it until we call compute()."),(0,o.yg)("pre",null,(0,o.yg)("code",{parentName:"pre",className:"language-python"},"dask_df.describe().compute()\n")),(0,o.yg)("pre",null,(0,o.yg)("code",{parentName:"pre",className:"language-python"},'dask_df["Categorical_3"].value_counts().compute().head()\n')),(0,o.yg)("p",null,'We also use value_counts() to count the number of occurrences of each unique value in the "Categorical_3" column. We trigger the operation with compute(), and head() shows us the most common values.'),(0,o.yg)("h3",{id:"-groupby"},"\u2728 Groupby"),(0,o.yg)("p",null,'Finally, let\'s use the groupby() function to group our data based on values in the "Categorical_8" column. Then we select the "Numeric_7" column and calculate the mean for each group.'),(0,o.yg)("p",null,'This is similar to how you might use \u2018groupby()\u2019 in pandas, but as you might have guessed, Dask does this lazily. We trigger the operation with compute(), and head() displays the average of the "Numeric_7" column for the first few groups.'),(0,o.yg)("pre",null,(0,o.yg)("code",{parentName:"pre",className:"language-python"},'dask_df.groupby("Categorical_8")["Numeric_7"].mean().compute().head()\n')),(0,o.yg)("h3",{id:"\ufe0f-lazy-evaluation"},"\u26a1\ufe0f Lazy evaluation"),(0,o.yg)("p",null,"Now, let\u2019s explore the use of the compute function at the end of each code block."),(0,o.yg)("p",null,"Dask evaluates code blocks in lazy mode compared to Pandas\u2019 eager mode, which returns results immediately."),(0,o.yg)("p",null,"To draw a parallel in cooking, lazy evaluation is like preparing ingredients and chopping vegetables in advance but only combining them to cook when needed. The compute function serves that purpose."),(0,o.yg)("p",null,"In contrast, eager evaluation is like throwing ingredients into the fire to cook as soon as they are ready. This approach ensures everything is ready to serve at once."),(0,o.yg)("p",null,"Lazy evaluation is key to Dask\u2019s excellent performance as it provides:"),(0,o.yg)("ol",null,(0,o.yg)("li",{parentName:"ol"},(0,o.yg)("strong",{parentName:"li"},"Reduced computation.")," Expressions are evaluated only when needed (when compute is called), avoiding unnecessary intermediate results that may not be used in the final result."),(0,o.yg)("li",{parentName:"ol"},(0,o.yg)("strong",{parentName:"li"},"Optimal resource allocation.")," Lazy evaluation avoids allocating memory or processing power to intermediate results that may not be required."),(0,o.yg)("li",{parentName:"ol"},(0,o.yg)("strong",{parentName:"li"},"Support for large datasets.")," This method processes data elements on-the-fly or in smaller chunks, enabling efficient utilization of memory resources.")),(0,o.yg)("p",null,"When the results of compute are returned, they are given as Pandas Series/DataFrames or NumPy arrays instead of native Dask DataFrames."),(0,o.yg)("pre",null,(0,o.yg)("code",{parentName:"pre",className:"language-python"},"type(dask_df)\n")),(0,o.yg)("pre",null,(0,o.yg)("code",{parentName:"pre"},"dask.dataframe.core.DataFrame\n")),(0,o.yg)("pre",null,(0,o.yg)("code",{parentName:"pre",className:"language-python"},'type(\n   dask_df[["Numeric_5", "Numeric_6", "Numeric_7"]].mean().compute()\n)\n')),(0,o.yg)("pre",null,(0,o.yg)("code",{parentName:"pre"},"pandas.core.series.Series\n")),(0,o.yg)("p",null,"The reason for this is that most data manipulation operations return only a subset of the original dataframe, taking up much smaller space. So, there won\u2019t be any need to use parallelism of Dask, and you continue the rest of your workflow either in pandas or NumPy."),(0,o.yg)("h4",{id:"-dask-bags-and-dask-delayed-for-unstructured-data"},"\ud83e\ude90 Dask Bags and Dask Delayed for Unstructured Data"),(0,o.yg)("p",null,"Dask Bags and Dask Delayed are two components of the Dask library that provide powerful tools for working with unstructured or semi-structured data and enabling lazy evaluation."),(0,o.yg)("p",null,"While in the past, tabular data was the most common, today\u2019s datasets often involve unstructured files such as images, text files, videos, and audio. Dask Bags provides the functionality and API to handle such unstructured files in a parallel and scalable manner."),(0,o.yg)("p",null,"For example, let\u2019s consider a simple illustration:"),(0,o.yg)("pre",null,(0,o.yg)("code",{parentName:"pre",className:"language-python"},'# Create a Dask Bag from a list of strings\nb = db.from_sequence(["apple", "banana", "orange", "grape", "kiwi"])\n\n# Filter the strings that start with the letter \'a\'\nfiltered_strings = b.filter(lambda x: x.startswith("a"))\n\n# Map a function to convert each string to uppercase\nuppercase_strings = filtered_strings.map(lambda x: x.upper())\n\n# Compute the result as a list\nresult = uppercase_strings.compute()\n\nprint(result)\n')),(0,o.yg)("pre",null,(0,o.yg)("code",{parentName:"pre"},"['APPLE']\n")),(0,o.yg)("p",null,"In this example, we create a Dask Bag b from a list of strings. We then apply operations on the Bag to filter the strings that start with the letter 'a' and convert them to uppercase using the filter() and map() functions, respectively. Finally, we compute the result as a list using the compute() method and print the output."),(0,o.yg)("p",null,"Now imagine that you can perform even more complex operations on billions of similar strings stored in a text file. Without the lazy evaluation and parallelism offered by Dask Bags, you would face significant challenges. "),(0,o.yg)("p",null,"As for Dask Delayed, it provides even more flexibility and introduces lazy evaluation and parallelism to various other scenarios. With Dask Delayed, you can convert any native Python function into a lazy object using the @dask.delayed decorator."),(0,o.yg)("p",null,"Here is a simple example:"),(0,o.yg)("pre",null,(0,o.yg)("code",{parentName:"pre",className:"language-python"},"%%time\n\nimport time\n@dask.delayed\ndef process_data(x):\n   # Simulate some computation\n   time.sleep(1)\n   return x**2\n\n\n# Generate a list of inputs\ninputs = range(1000)\n\n# Apply the delayed function to each input\nresults = [process_data(x) for x in inputs]\n\n# Compute the results in parallel\ncomputed_results = dask.compute(*results)\n")),(0,o.yg)("pre",null,(0,o.yg)("code",{parentName:"pre"},"CPU times: user 260 ms, sys: 68.1 ms, total: 328 ms\nWall time: 32.2 s\n")),(0,o.yg)("p",null,"In this example, we define a function process_data decorated with @dask.delayed. The function simulates some computational work by sleeping for 1 second and then returning the square of the input value."),(0,o.yg)("p",null,"Without parallelism, performing this computation on 1000 inputs would have taken more than 1000 seconds. However, with Dask Delayed and parallel execution, the computation only took about 42.1 seconds."),(0,o.yg)("p",null,"This example demonstrates the power of parallelism in reducing computation time by efficiently distributing the workload across multiple cores or workers."),(0,o.yg)("p",null,"That\u2019s what parallelism is all about. for more information see ",(0,o.yg)("a",{parentName:"p",href:"https://docs.dask.org/en/stable/"},"https://docs.dask.org/en/stable/")),(0,o.yg)("pre",null,(0,o.yg)("code",{parentName:"pre",className:"language-python"},"")))}m.isMDXComponent=!0}}]);