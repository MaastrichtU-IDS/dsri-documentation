---
id: deploy-spark
title: Deploy a Spark cluster
---

Apache Spark can be deployed on the DSRI. Using a template allowing to configure the Spark cluster.

> Root access it required in the DSRI project.

See [vemonet/spark-openshift](https://github.com/vemonet/spark-openshift) to create the template in your project.

## Create the template

Clone the [vemonet/spark-openshift](https://github.com/vemonet/spark-openshift) repository:

```bash
git clone https://github.com/vemonet/spark-openshift.git
cd spark-openshift
```

Login with the `oc` command line tool and go to your project:

```shell
oc project <my_project>
```

Create the template:

```shell
oc create -f spark-template-existing-pvc.yml
```

## Deploy a Spark cluster

Go to the DSRI OpenShift web UI catalog and run the `Apache Spark` applications.

You will be prompted various parameters to configure your Spark cluster resources.

> Only 1 Spark cluster should be deployed by project.

## Delete a running Spark cluster

Get all objects generated by the template:

```bash
oc get all --selector app=spark
```

Delete all objects generated by the template:

```bash
oc delete all --selector app=spark
oc delete secret --selector app=spark
oc delete configmaps --selector app=spark
```

## Delete the Spark template

In case you want to delete or update the Spark template:

```shell
oc delete -f spark-template-existing-pvc.yml
```

